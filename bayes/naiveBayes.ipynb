{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯(Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 介绍\n",
    "\n",
    "- 基于贝叶斯计算预测函数时, 都要基于此公式: \n",
    "$$ P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$$\n",
    "$$ P(\\text{类别} \\mid \\text{特征})=\\frac{P(\\text{特征} \\mid \\text{类别}) \\times P(\\text{类别})}{P(\\text{特征})} $$\n",
    "- $ P(y|x)$ 是给定参数x后y的概率, 也就是对应了预测函数, 也是所谓的后验概率\n",
    "- $ P(x|y)$ 是给定结果y后x的概率, 是根据结果推测最可能的参数, 也就是似然性\n",
    "- $ P(y)$ 是结果本身的为某值的概率, 是先验概率\n",
    "- $ P(x)$ 是参数发生的概率, 是先验概率, 一般可视为常量\n",
    "\n",
    "- 朴素贝叶斯的假设 参数条件是相互独立的, 即: $ x_i x_j | y(i \\neq j)$, 目的是为了简化运算, 每个参数条件都可以单独计算分布, 最后合并\n",
    "\n",
    "- 因此, 模型有: $$ P(y|x) = \\prod_{i=1}^{N}(P(x_i|y)P(y))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝叶斯估计\n",
    "\n",
    "- 在做极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 0 的情况。会影响后验概率的计算结果，使得分类产生偏差。解决这一问题最好的方法就是采用贝叶斯估计\n",
    "$$ P(y_{i}=c_{k})=\\frac{\\sum_{i=1}^{N}I(y_{i}=c_{k})+\\lambda}{N+k\\lambda}$$\n",
    "\n",
    "- 当 $\\lambda=0$ 时就是极大似然估计\n",
    "- 当 $\\lambda=1$ 时称为拉普拉斯平滑, 每一个分类都会加上1, 分母k为总分类数, 分类指某一个维度能有多少个项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0.5333333333333333, 'B': 0.4666666666666667}\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'r|A': 0.5,\n",
       " 'm|A': 0.375,\n",
       " 'r|B': 0.14285714285714285,\n",
       " 'm|B': 0.42857142857142855}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def create_data():\n",
    "    # 生成示例数据\n",
    "    data = {\n",
    "        \"x\": [\"r\",\"g\",\"r\",\"b\",\"g\",\"g\",\"r\",\"r\",\"b\",\"g\",\"g\",\"r\",\"b\",\"b\",\"g\",],\n",
    "        \"y\": [\"m\",\"s\",\"l\",\"s\",\"m\",\"s\",\"m\",\"s\",\"m\",\"l\",\"l\",\"s\",\"m\",\"m\",\"l\",],\n",
    "        \"labels\": [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",],\n",
    "    }\n",
    "    data = pd.DataFrame(data, columns=[\"labels\", \"x\", \"y\"])\n",
    "    return data\n",
    "\n",
    "data = create_data()\n",
    "\n",
    "\n",
    "def get_P_labels(labels):\n",
    "    # P(\\text{种类}) 先验概率计算\n",
    "    labels = list(labels)  # 转换为 list 类型\n",
    "    P_label = {}  # 设置空字典用于存入 label 的概率\n",
    "    for label in labels:\n",
    "        # 统计 label 标签在标签集中出现的次数再除以总长度\n",
    "        P_label[label] = labels.count(label) / float(\n",
    "            len(labels)\n",
    "        )  # p = count(y) / count(Y)\n",
    "    return P_label\n",
    "\n",
    "\n",
    "P_labels = get_P_labels(data[\"labels\"])\n",
    "print(P_labels)\n",
    "\n",
    "# 将 data 中的属性切割出来，即 x 和 y 属性\n",
    "train_data = np.array(data.iloc[:, 1:])\n",
    "\n",
    "labels = data[\"labels\"]\n",
    "label_index = []\n",
    "# 遍历所有的标签，这里就是将标签为 A 和 B 的数据集分开，label_index 中存的是该数据的下标\n",
    "for y in P_labels.keys():\n",
    "    temp_index = []\n",
    "    # enumerate 函数返回 Series 类型数的索引和值，其中 i 为索引，label 为值\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == y:\n",
    "            temp_index.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    label_index.append(temp_index)\n",
    "# 遍历 train_data 中的第一列数据，提取出里面内容为r的数据\n",
    "x_index = [\n",
    "    i for i, feature in enumerate(train_data[:, 0]) if feature == \"r\"\n",
    "]  # 效果等同于求类别索引中 for 循环\n",
    "print(label_index)\n",
    "\n",
    "\n",
    "def get_P_fea_lab(P_label, features, data):\n",
    "    # P(\\text{特征}∣种类) 先验概率计算\n",
    "    # 该函数就是求 种类为 P_label 条件下特征为 features 的概率\n",
    "    P_fea_lab = {}\n",
    "    train_data = data.iloc[:, 1:]\n",
    "    train_data = np.array(train_data)\n",
    "    labels = data[\"labels\"]\n",
    "    # 遍历所有的标签\n",
    "    for each_label in P_label.keys():\n",
    "        # 上面代码的另一种写法，这里就是将标签为 A 和 B 的数据集分开，label_index 中存的是该数据的下标\n",
    "        label_index = [i for i, label in enumerate(labels) if label == each_label]\n",
    "\n",
    "        # 遍历该属性下的所有取值\n",
    "        # 求出每种标签下，该属性取每种值的概率\n",
    "        for j in range(len(features)):\n",
    "            # 筛选出该属性下属性值为 features[j] 的数据\n",
    "            feature_index = [\n",
    "                i\n",
    "                for i, feature in enumerate(train_data[:, j])\n",
    "                if feature == features[j]\n",
    "            ]\n",
    "\n",
    "            # set(x_index)&set(y_index) 取交集，得到标签值为 each_label,属性值为 features[j] 的数据集合\n",
    "            fea_lab_count = len(set(feature_index) & set(label_index))\n",
    "            key = str(features[j]) + \"|\" + str(each_label)  # 拼接字符串\n",
    "\n",
    "            # 计算先验概率\n",
    "            # 计算 labels 为 each_label下，featurs 为 features[j] 的概率值\n",
    "            P_fea_lab[key] = fea_lab_count / float(len(label_index))\n",
    "    return P_fea_lab\n",
    "\n",
    "\n",
    "features = [\"r\", \"m\"]\n",
    "get_P_fea_lab(P_labels, features, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"A|['r', 'm']\": 0.1, \"B|['r', 'm']\": 0.02857142857142857}\n",
      "{'A': 0.1, 'B': 0.02857142857142857}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(data, features):\n",
    "    # 朴素贝叶斯分类器\n",
    "    # 求 labels 中每个 label 的先验概率\n",
    "    labels = data[\"labels\"]\n",
    "    # 这里也就是求 P（B），P_label 为一个字典，存的是每个 B 对应的 P(B)\n",
    "    P_label = get_P_labels(labels)\n",
    "    P_fea_lab = get_P_fea_lab(P_label, features, data)  # 这里是在求 P（A|B）\n",
    "\n",
    "    P = {}\n",
    "    P_show = {}  # 后验概率\n",
    "    for each_label in P_label:\n",
    "        P[each_label] = P_label[each_label]\n",
    "        # 遍历每个标签下的每种属性\n",
    "        for each_feature in features:\n",
    "            # 拼接字符串为 P(B/A) 用于字典的键值\n",
    "            key = str(each_label) + \"|\" + str(features)\n",
    "            # 计算 P(B/A) = P(B) * P(A/B) 因为所有的后验概率，分母相同。因此，在计算时可以忽略掉。\n",
    "            P_show[key] = (\n",
    "                P[each_label] * P_fea_lab[str(each_feature) + \"|\" + str(each_label)]\n",
    "            )\n",
    "            # 把刚才算的概率放到 P 列表里面，这个 P 列表的键值变成了标签。\n",
    "            # 这样做的目的，其实是为了在后面取最大时，取出就是标签，而不是 标签|特征\n",
    "            P[each_label] = (\n",
    "                P[each_label] * P_fea_lab[str(each_feature) + \"|\" + str(each_label)]\n",
    "            )\n",
    "    # 输出 P_show 和 P 观察，发现他们的概率值没有变，只是字典的 key 值变了\n",
    "    print(P_show)\n",
    "    print(P)\n",
    "    features_label = max(P, key=P.get)  # 概率最大值对应的类别\n",
    "    return features_label\n",
    "\n",
    "classify(data, [\"r\", \"m\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 邮件过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\issuser\\AppData\\Local\\Temp\\ipykernel_1080\\3783788360.py:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_table is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_table('E:/vscode/file/trec06c/full/index', header=None,\n",
      "C:\\Users\\issuser\\AppData\\Local\\Temp\\ipykernel_1080\\3783788360.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = data.replace(['spam', 'ham'], [0, 1])  # 0 替代 spam，1 替代 ham\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'上海 培训 课程 财务 纠淼 沙盘 模拟 财务 课程 背景 一位 管理 技术人员 懂得 技术 角度 衡量 合算 方案 也许 却是 财务 陷阱 表面 赢利 亏损 使经 营者 接受 技术手段 财务 运作 相结合 每位 管理 技术人员 老板 角度 思考 规避 财务 陷阱 管理决策 目标 一致性 课程 沙盘 模拟 案例 分析 企业 管理 技术人员 财务管理 知识 利用 财务 信息 改进 管理决策 管理 效益 最大化 学习 课程 会计 财务管理 提高 日常 管理 活动 财务 可行性 业绩 评价 方法 评估 业绩 实施 科学 业绩考核 合乎 财务 墓芾 老板 思维 同步 分析 关键 业绩 指标 战略规划 预算 企业 管理 重心 管理 系统性 课程 大纲 财务 工作 内容 作用 财务会计 财务 专家 思维 模式 财务 工作 内容 管理者 利用 财务 管理 决策 阅读 分析 财务报表 会计报表 损益表 阅读 分析 资产 负债表 阅读 分析 资金 流量 现金流量 阅读 分析 会计报表 之间 关系 会计报表 读懂 企业 状况 案例 分析 报表 判断 企业 业绩 水平 财务 手段 成本 控制 产品成本 概念 本浚利 分析 标准 成本 制度 成本 控制 作用 目标 成本法 控制 产品成本 保证 利润 水平 作业 成本法 管理 分析 实施 精细 成本 管理 沉没 成本 机会成本 正确 决策 改善 采购 生产 环节 运作 改良 企业 整体 财务状况 综合 案例 分析 财务 尚械 墓芾 醴桨 管理 技术 方案 可行性 分析 产品开发 财务 可行性 分析 产品 增产 减产 财务 可行性 分析 生产 设备 改造 更新 决策分析 投资 项目 现金流 分析 投资 项目 评价 方法 现值 法分析 资金 时间 价值 分析 综合 案例 演练 公司 费用 控制 公司 费用 控制 费用 方法 影响 费用 因素 分析 成本 中心 费用 控制 利润 中心 业绩考核 投资 中心 业绩 评价 利用 财务 数据分析 改善 绩效 公司财务 分析 核心 思路 关键 财务指标 解析 盈利 能力 分析 资产 回报率 股东权益 回报率 资产 流动 速率 风险 指数 分析 流动比率 负债 权益 比率 营运 偿债 能力 财务报表 综合 解读 综合 财务 信息 透视 公司 运作 水平 案例 分析 上市公司 财务状况 分析 评价 企业 运营 管理 沙盘 模拟 模拟 体验式 教学 小组 模拟 公司 生产 销售 财务 分析 过程 钥怪 邢嗷逖 企业 乐趣 讲师 学员 分享 解决问题 模型 工具 学员 身同 转化 导师 简介 管理 工程硕士 高级 炯檬 国际 职业 培训师 认证 职业 培训师 历任 跨国公司 生产 负责人 工业 工程 管理 会计 分析师 营运 总监 高级 管理 职务 多年 担任 价值 工程 审稿人 辽宁省 营口市 商业银行 独立 职务 企业 管理 研究 王老师 技术 成本 控制 管理 会计 决策 课程 讲授 松下 可口可乐 康师傅 果汁 雪津 啤酒 吉百利 食品 冠捷 电子 明达 塑胶 正新 橡胶 美国 集团 广上 科技 美的 空调 中兴通讯 京信 通信 联想 电脑 材料 中国 公司 艾克森 金山 石化 中国 化工 进出口 公司 正大 集团 大福 饲料 厦华 集团 灿坤 股份 东金 电子 太原 钢铁集团 深圳 开发 科技 大冷 运输 制冷 三洋 华强 名企 提供 项目 辅导 专题 培训 王老师 授课 狙榉 风格 幽默诙谐 逻辑 清晰 过程 互动 案例 生动 深受 学员 喜爱 授课 时间 地点 周六日 上海 课程 费用 元人 包含 培训 费用 午餐 证书 资料 优惠 三人 参加 赠送 名额 联系人 电话传真'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "data = pd.read_table('E:/vscode/file/trec06c/full/index', header=None,\n",
    "                     encoding='gb2312', delim_whitespace=True)\n",
    "df = data.replace(['spam', 'ham'], [0, 1])  # 0 替代 spam，1 替代 ham\n",
    "df = df.replace(regex=[\"\\..\"], value='E:/vscode/file/trec06c')  # 替换掉文件路径\n",
    "df = df.sample(len(df), random_state=1, )[:10000]  # 打乱样本并取前 1 万条数据\n",
    "df.groupby(0).count()  # 统计样本\n",
    "\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    # 加载停用词\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = [line.strip('\\n') for line in f.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stopwords('E:/vscode/file/trec06c/stopwords.txt')\n",
    "stopwords\n",
    "\n",
    "def clean_str(line):\n",
    "    # 清理邮件，替换不需要的字符串\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()\n",
    "\n",
    "\n",
    "def process(file_path, test_mode=False):\n",
    "    # 清洗一封邮件\n",
    "    '''\n",
    "    - file_path: 文本文件路径\n",
    "    - test_mode: 测试模式，后文我们会将一个字符串写入文件(utf-8 编码)，而训练文件以 GBK 编码，\n",
    "                 如果自己实现分类，请注意编码格式，通常为 utf-8\n",
    "    - return: words, 处理、分词之后的有效词语\n",
    "    '''\n",
    "    words = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            if not test_mode:\n",
    "                line = line.strip().decode(\"gbk\", 'ignore')\n",
    "            else:\n",
    "                line = line.strip().decode(\"utf-8\", 'ignore')\n",
    "            line = clean_str(line)\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            seg_list = list(jieba.cut(line, cut_all=False))\n",
    "            for x in seg_list:\n",
    "                if len(x) <= 1:\n",
    "                    continue\n",
    "                if x in stopwords:\n",
    "                    continue\n",
    "                words.append(x)\n",
    "    return words\n",
    "\n",
    "words = process('E:/vscode/file/trec06c/data/000/000')\n",
    "\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:35<00:00, 283.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37029</th>\n",
       "      <td>1</td>\n",
       "      <td>E:/vscode/file/trec06c/data/123/129</td>\n",
       "      <td>[恋爱, 第三次, 告诉, 再见面, 时间, 我要, 考研, 考到, 北京, 是否是, 喜欢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>0</td>\n",
       "      <td>E:/vscode/file/trec06c/data/007/157</td>\n",
       "      <td>[欣欣, 签约, 推出, 中国, 第一个, 彩铃, 歌手, 稀稀, 龙乐, 公司, 签约, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50881</th>\n",
       "      <td>1</td>\n",
       "      <td>E:/vscode/file/trec06c/data/169/181</td>\n",
       "      <td>[男生, 思路, 简单, 心痛, 直说, 原因, 不让, 担心, 他累, 不去, 撒娇, 撒...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>0</td>\n",
       "      <td>E:/vscode/file/trec06c/data/036/043</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>0</td>\n",
       "      <td>E:/vscode/file/trec06c/data/015/189</td>\n",
       "      <td>[本港, 会计师, 权威机构, 香港, 瑞丰, 会计师, 事务所, 注册, 海外, 国际, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                    1  \\\n",
       "37029  1  E:/vscode/file/trec06c/data/123/129   \n",
       "2257   0  E:/vscode/file/trec06c/data/007/157   \n",
       "50881  1  E:/vscode/file/trec06c/data/169/181   \n",
       "10843  0  E:/vscode/file/trec06c/data/036/043   \n",
       "4689   0  E:/vscode/file/trec06c/data/015/189   \n",
       "\n",
       "                                                   words  \n",
       "37029  [恋爱, 第三次, 告诉, 再见面, 时间, 我要, 考研, 考到, 北京, 是否是, 喜欢...  \n",
       "2257   [欣欣, 签约, 推出, 中国, 第一个, 彩铃, 歌手, 稀稀, 龙乐, 公司, 签约, ...  \n",
       "50881  [男生, 思路, 简单, 心痛, 直说, 原因, 不让, 担心, 他累, 不去, 撒娇, 撒...  \n",
       "10843                                                 []  \n",
       "4689   [本港, 会计师, 权威机构, 香港, 瑞丰, 会计师, 事务所, 注册, 海外, 国际, ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()  # 使用 tqdm 显示进度\n",
    "# 将 apply 函数替换为 progress_apply 以使用 tqdm 显示处理进度\n",
    "df[\"words\"] = df[1].progress_apply(process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x151fb64c3a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# 移除一些不必要的警告\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 导入上面保存的分词数组\n",
    "data = df[\"words\"]\n",
    "\n",
    "# 训练 Word2Vec 浅层神经网络模型\n",
    "w2v_model = Word2Vec(vector_size=100, min_count=10)\n",
    "w2v_model.build_vocab(data)\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=5)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e0b101d3fe43cda7f6f8c977e754b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_vec(text):\n",
    "    # 对每个句子的进行词向量求和计算\n",
    "    vec = np.zeros(100).reshape((1, 100))\n",
    "    for word in text:\n",
    "        try:\n",
    "            # 得到句子中每个词的词向量并累加在一起\n",
    "            vec += w2v_model.wv.get_vector(word).reshape((1, 100))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec\n",
    "\n",
    "# 将词向量保存为 Ndarray\n",
    "data_vec = np.concatenate([sum_vec(z) for z in tqdm_notebook(data)])\n",
    "data_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 100), (2000, 100), (8000,), (2000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_data = data_vec\n",
    "label_data = df[0].values\n",
    "# 分割数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_data, label_data, test_size=0.2, random_state=4\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()  # 定义伯努利模型分类器\n",
    "model.fit(X_train, y_train)  # 模型训练\n",
    "y_pred = model.predict(X_test)  # 模型预测\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
